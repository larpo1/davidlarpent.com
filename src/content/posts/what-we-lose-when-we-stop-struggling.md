---
title: The Unbearable Lightness of Prompting
date: 2026-01-30T00:00:00.000Z
description: >-
  On skill atrophy, the joy of difficulty, and whether it is too late to save
  what we love.
draft: false
---
  <p>Anthropic just published a study showing that developers who use AI assistance score 17% lower on understanding the code they just wrote. They finished the task slightly faster. They apparently learned almost nothing.</p>
<p>The researchers watched how people worked. Some handed everything to the AI and breezed through. Others asked the AI questions, then wrote the code themselves. The first group was quickest. The second group actually understood what they’d built.</p>
<p>This is the product working exactly as designed. We built tools to help people go faster. They went faster. They just didn’t go anywhere.</p>
<hr>
<h2 id="the-speed-trap">The Speed Trap</h2>
<p>We’ve confused two different things: making decisions faster and making decisions better.</p>
<p>Sometimes speed is the game. A trader has milliseconds. A pilot in an emergency has seconds. An ER doctor triaging patients needs answers now. In these contexts, give people all the AI support you can. Faster is better because the clock is the constraint.</p>
<p>But most decisions aren’t like that.</p>
<p>Most decisions benefit from sitting with them. Turning them over. Feeling the friction of competing considerations. A product manager deciding what to build next. A leader choosing who to promote. A parent figuring out how to handle a difficult conversation with a teenager.</p>
<p>These decisions don’t need speed. They need the slow work of judgment forming. And judgment doesn’t form when you skip to the answer.</p>
<p>The Anthropic study makes this concrete. Developers who let AI do the thinking finished a bit faster. But when tested on what they’d learned minutes later, they bombed. The gap was biggest on debugging questions. The exact skill you need when something goes wrong. The exact moment when you can’t just ask the AI, because the AI is what got you here.</p>
<hr>
<h2 id="the-formation-problem">The Formation Problem</h2>
<p>Here’s the thing about neural pathways: they require reps.</p>
<p>You don’t learn to write by reading. You learn by writing badly, repeatedly, until you write less badly. You don’t learn to diagnose problems by watching someone else diagnose them. You learn by being wrong, often, and slowly getting less wrong.</p>
<p>This is annoying. We’d prefer to skip the part where we’re bad at things. AI offers that skip. And the skip feels great, right up until you need the skill you never built.</p>
<p>The pilots who hand-fly the least are worst at hand-flying when the automation fails. The doctors most reliant on diagnostic algorithms are slowest to catch what the algorithm misses. The coders who outsource debugging to AI can’t debug when the AI is confused. We’ve known this pattern for decades. Lisanne Bainbridge called it the “ironies of automation” back in 1983: the more advanced the system, the more crucial the human contribution when things go sideways, and the less capable the human is of making it.</p>
<p>But there’s something else here that bothers me more than the competence question.</p>
<p>Humans who never formed the skills aren’t just less useful. They’re less interesting.</p>
<p>I don’t mean this as snobbery. I mean it literally. A person who has wrestled with hard problems has opinions about them. They’ve developed instincts, heuristics, preferences. They can surprise you with an angle you hadn’t considered. They’re <em>fun to talk to</em> about the thing they’ve struggled with.</p>
<p>A person who’s always had the answer handed to them has nothing to say. They can tell you what the AI told them. That’s it.</p>
<p>We’re not just risking a less capable workforce. We’re risking less capable people. Less interesting dinner companions. Less useful collaborators. Less able to hold a surprising opinion because they’ve never had to form one.</p>
<hr>
<h2 id="the-joy-problem">The Joy Problem</h2>
<p>I paint. Oils, mostly. I also make music, badly, for my own amusement.</p>
<p>Here’s the thing: I don’t paint to have paintings.</p>
<p>If I wanted paintings, I could buy paintings. Or generate them. A few sentences to Midjourney and I’d have something technically impressive on my wall in under a minute.</p>
<p>I paint because of what happens when I paint. The focus. The frustration when the colour isn’t right. The weird satisfaction when something starts to work. The way time disappears. The moment where your hand does something your conscious mind didn’t plan and it’s somehow exactly right.</p>
<p>That stuff doesn’t happen when you type a prompt. The output might look similar. The experience is nothing alike.</p>
<p>Humans have made art since we had hands and walls. Cave paintings from 36,000 years ago. Not because our ancestors needed interior decorating. Because there’s something in us that wants to make things. Wants to struggle with materials. Wants to leave a mark that says <em>I was here, and I made this, and it was hard</em>.</p>
<p>The difficulty is the point.</p>
<p>Not difficulty for its own sake. Difficulty as the texture of engagement. The thing that makes the breakthrough feel like a breakthrough. The resistance that gives your effort meaning.</p>
<p>What happens when we commoditise the output?</p>
<p>When the thing you can make in 30 seconds with a prompt is indistinguishable from the thing I sweated over for 30 hours, something strange happens to the value of both. Not the market value. The felt value. The meaning.</p>
<p>I’m not worried about AI putting artists out of work. I’m worried about AI putting artists out of the <em>experience</em> of being artists. Making the struggle feel pointless. Making the hard-won skill feel like a quaint affectation.</p>
<p>I’m worried we build a world where we can have anything we want, effortlessly, and none of it means anything.</p>
<hr>
<h2 id="the-choice-we-actually-face">The Choice We Actually Face</h2>
<p>None of this is an argument against AI. I use it constantly. I’m running an AI assistant right now that helps me think, research, write, and manage my work. It makes me better at my job in ways I wouldn’t want to give up.</p>
<p>But I’m trying to be deliberate about <em>which</em> parts of my job I hand over.</p>
<p>The stuff that’s mechanical, repetitive, or where speed genuinely matters? Take it. The stuff where the struggle is the learning? That I keep. Not because AI can’t do it. Because doing it myself is how I stay sharp. How I stay interesting. How I stay <em>me</em>.</p>
<p>This might sound like an individual choice, a matter of personal discipline. But it’s not just that. It’s a design choice. A leadership choice. A parenting choice.</p>
<p>If you run a company, you’re making decisions every day about which cognitive work your people do themselves and which they outsource. Get this wrong and you’ll have a team that executes fast and thinks slow. That can ship but can’t adapt. That looks productive right up until the world changes and nobody knows what to do.</p>
<p>If you’re building AI products (and I am), you’re deciding whether to replace human judgment or augment it. Whether to give people answers or help them find answers. Whether to make users faster or make them better. These aren’t the same thing. Sometimes they’re opposites.</p>
<p>If you’re raising kids, you’re deciding what struggles to protect. Your child can use ChatGPT to write their essay. The essay might be good. But if they never learn to structure an argument, to sit with the discomfort of not knowing what to say, to push through the part where writing is hard… what have you taught them?</p>
<p>That the point of writing is to have written?</p>
<p>That difficulty is always a problem to be solved rather than sometimes the whole point?</p>
<hr>
<h2 id="what-do-we-want-to-be-good-at">What Do We Want To Be Good At?</h2>
<p>Here’s the question I keep coming back to:</p>
<p>If we outsource everything we can outsource, what’s left?</p>
<p>What do we keep because the keeping is the point?</p>
<p>I don’t think there’s one right answer. Different people, different teams, different cultures will draw the line in different places. But I think we need to draw it consciously, rather than letting the logic of efficiency draw it for us.</p>
<p>For me, the line looks something like this:</p>
<p><strong>Give to AI:</strong> Research, summarisation, first drafts I’m going to heavily rewrite, scheduling, formatting, the mechanical parts of coding, anything where I already know what I think and just need it executed.</p>
<p><strong>Keep for myself:</strong> The hard thinking about what to build. The creative work I do for joy. The conversations that matter. The writing where I’m trying to figure out what I believe. The reps that build the skills I want to have.</p>
<p><strong>Protect for my team:</strong> The struggles that make them grow. The decisions that build their judgment. The friction that turns juniors into seniors.</p>
<p><strong>Protect for my kids:</strong> The experience of not knowing something and having to figure it out. The satisfaction of making something hard work. The understanding that difficulty is often the texture of meaning, not an obstacle to it.</p>
<hr>
<h2 id="the-species-question">The Species Question</h2>
<p>Maybe this sounds grandiose. A guy who paints on weekends worrying about the fate of human meaning.</p>
<p>But I don’t think it is.</p>
<p>We’re making choices right now, as a species, about what we want to be good at. About what parts of human experience we preserve and what parts we optimise away. About whether we want to live in a world of effortless abundance or one where effort still means something.</p>
<p>I don’t know the right answer. I’m suspicious of anyone who claims to.</p>
<p>But I know the wrong answer is not thinking about it at all. Letting the question get answered by default, by whatever’s most efficient, by the path of least resistance.</p>
<p>The path of least resistance leads somewhere. I’m just not sure we want to go there.</p>
<hr>        

[^1]: Shen, J.H. & Tamkin, A. (2026). "How AI Impacts Skill Formation." arXiv:2601.20245. The study found participants using AI assistance scored 50% on comprehension tests vs. 67% for the control group. [Full paper](https://arxiv.org/abs/2601.20245)

[^2]: The FAA has documented this extensively. A 2013 safety alert noted that pilots who regularly use automated systems "ichever have fewer opportunities to maintain and improve" manual flying skills, creating risks when automation fails or must be overridden. See FAA SAFO 13002.

[^3]: Studies on automation bias in medicine show clinicians can become over-reliant on clinical decision support systems. See Goddard, K., Roudsari, A., & Wyatt, J.C. (2012). "Automation bias: a systematic review of frequency, effect mediators, and mitigators." Journal of the American Medical Informatics Association, 19(1), 121-127.

[^4]: Bainbridge, L. (1983). "Ironies of automation." Automatica, 19(6), 775-779. The paper's central argument: "the more advanced a control system is, so the more crucial may be the contribution of the human operator, but the less opportunity the operator has to practice these skills." Still cited heavily 40+ years later.

[^5]: The Chauvet Cave paintings in France are dated to approximately 30,000-36,000 years ago, making them among the oldest known figurative art. See Clottes, J. (2003). "Chauvet Cave: The Art of Earliest Times." University of Utah Press.
