---
title: The Unbearable Lightness of Prompting
date: 2026-01-30T00:00:00.000Z
description: >-
  On skill atrophy, the joy of difficulty, and whether it is too late to save
  what we love.
draft: false
---
I wrote this the old fashioned way. Partly for my kids, partly for myself.  

Anthropic just published a study showing that developers who use AI assistance score 17% lower on understanding the code they just wrote. They finished the task slightly faster. They apparently learned almost nothing.

The researchers watched how people worked. Some handed everything to the AI and breezed through. Others asked the AI questions, then wrote the code themselves. The first group was quickest, but only the second group understood what they’d built.

## The Speed Trap

Sometimes speed is the game. A trader has milliseconds. A pilot in an emergency has seconds. An ER doctor triaging patients needs answers now. In these contexts, give people all the AI support you can. Faster is better because the clock is the constraint.

But most decisions aren’t like that.

Most decisions benefit from sitting with them. Turning them over. Feeling the friction of competing considerations. A product manager deciding what to build next. A leader choosing who to promote. A parent figuring out how to handle a difficult conversation with a teenager.

These decisions don’t need speed. They need the slow work of judgment forming. And judgment doesn’t form when you skip to the answer.

The Anthropic study makes this concrete. Developers who let AI do the thinking finished a bit faster. But when tested on what they’d learned minutes later, they were hopeless. The gap was biggest on debugging questions. The exact skill you need when something goes wrong.

## The Formation Problem

Neural pathways are formed by reps and struggle. You don’t learn to write by reading. You learn by writing badly, repeatedly, until you write less badly. You don’t learn to diagnose problems by watching someone else diagnose them. You learn by being wrong, often, and slowly getting less wrong.

This is annoying. We’d prefer to skip the part where we’re bad at things. AI offers that skip. And the skip feels great, right up until you need the skill you never built.

The pilots who hand-fly the least are worst at hand-flying when the automation fails. The doctors most reliant on diagnostic algorithms are slowest to catch what the algorithm misses. The coders who outsource debugging to AI can’t debug when the AI is confused. We’ve known this pattern for decades. Lisanne Bainbridge called it the “ironies of automation” back in 1983: the more advanced the system, the more crucial the human contribution when things go sideways, and the less capable the human is of making it.

But there’s something else here that bothers me more than the competence question.

A person who has wrestled with hard problems often has interesting opinions about them. They’ve developed instincts, heuristics, preferences. They can surprise you with an angle you hadn’t considered. They’re _engaging to talk to_ about the thing they’ve struggled with.

A person who’s always had the answer handed to them …well, isn’t. They can tell you what the AI told them. That’s it.

We’re not just risking a less capable workforce. We’re risking less capable people. Less interesting dinner chat. Less useful collaborators. Less able to hold a surprising opinion because they’ve never had to form one.

## The Joy Problem

I paint. Oils, mostly. I also make music, badly, for my own amusement.  

I don’t paint to have paintings. I paint because of what happens when I paint.  The engrossing frustration when the colour isn’t right. The weird satisfaction when something starts to work. The way the hyper focus kicks in and time disappears. The moment where the hand does something the conscious mind didn’t plan and it’s somehow feels exactly right. 

That stuff doesn’t happen when you type a prompt. The output might look similar. The experience is nothing alike.

Humans have made art since we had hands and walls. Cave paintings from 36,000 years ago. Not because our ancestors needed interior decorating. Because there’s something in us that wants to make things. Wants to struggle with materials. Wants to leave a mark that says _I was here, and I made this, and it was hard_.

The difficulty is the point.

Not difficulty for its own sake. Difficulty as the texture of engagement. The thing that makes the breakthrough feel like a breakthrough. The resistance that gives your effort meaning.

What happens when we commoditise the output?

When the thing you can make in 30 seconds with a prompt is indistinguishable from the thing I sweated over for 30 hours, something strange happens to the value of both. Not the market value. The felt value. The meaning.

I’m not worried about AI putting artists out of work. I’m worried about AI putting artists out of the _experience_ of being artists. Making the struggle feel pointless. 

I’m worried we build a world where we can have anything we want, effortlessly, and none of it means anything.

## The Choice

None of this is an argument against AI. I use it constantly. I’m running an AI assistant right now that helps me think, research, write, and manage my work. It makes me better at my job in ways I wouldn’t want to give up.

But I’m trying to be deliberate about _which_ parts of my job I hand over.

The stuff that’s mechanical, repetitive, or where speed genuinely matters? Take it. The stuff where the struggle is the learning? That I keep.

This might sound like an individual choice, a matter of personal discipline. But it’s not just that. It’s a design choice. A leadership choice. A parenting choice.

If you run a company, you’re making decisions every day about which cognitive work your people do themselves and which they outsource. Get this wrong and you’ll have a team that executes fast and thinks slow. That can ship but can’t adapt. That looks productive right up until the world changes and nobody knows what to do.

If you’re building AI products (and I am), you’re deciding whether to replace human judgment or augment it. Whether to give people answers or help them find answers. Whether to make users faster or make them better. These aren’t the same thing. Sometimes they’re opposites.

If you’re raising kids, you’re deciding what struggles to protect. Your child can use ChatGPT to write their essay. The essay might be good. But if they never learn to structure an argument, to sit with the discomfort of not knowing what to say, to push through the part where writing is hard… what have you taught them?

That the point of writing is to have written?

## What Do We Want To Be Good At?

Here’s the question I keep coming back to:

If we outsource everything we can outsource, what’s left?

What do we keep because the keeping is the point?

I don’t think there’s one right answer. Different people, different teams, different cultures will draw the line in different places. But I think we need to draw it consciously, rather than letting the logic of efficiency draw it for us.

For me, the line looks something like this:

**Give to AI:** Research, summarisation, first drafts I’m going to heavily rewrite, scheduling, formatting, the mechanical parts of coding, anything where I already know what I think and just need it executed.

**Keep for myself:** The hard thinking about what to build. The creative work I do for joy. The conversations that matter. The writing where I’m trying to figure out what I believe. The reps that build the skills I want to have.

**Protect for my team:** The struggles that make them grow. The decisions that build their judgment. The friction that turns juniors into seniors.

**Protect for my kids:** The experience of not knowing something and having to figure it out. The satisfaction of making something hard work. The understanding that difficulty is often the texture of meaning, not an obstacle to it.

* * *

[^1]: Shen, J.H. & Tamkin, A. (2026). "How AI Impacts Skill Formation." arXiv:2601.20245. The study found participants using AI assistance scored 50% on comprehension tests vs. 67% for the control group. [Full paper](https://arxiv.org/abs/2601.20245)

[^2]: The FAA has documented this extensively. A 2013 safety alert noted that pilots who regularly use automated systems "have fewer opportunities to maintain and improve" manual flying skills, creating risks when automation fails or must be overridden. See FAA SAFO 13002.

[^3]: Studies on automation bias in medicine show clinicians can become over-reliant on clinical decision support systems. See Goddard, K., Roudsari, A., & Wyatt, J.C. (2012). "Automation bias: a systematic review of frequency, effect mediators, and mitigators." Journal of the American Medical Informatics Association, 19(1), 121-127.

[^4]: Bainbridge, L. (1983). "Ironies of automation." Automatica, 19(6), 775-779. The paper's central argument: "the more advanced a control system is, so the more crucial may be the contribution of the human operator, but the less opportunity the operator has to practice these skills." Still cited heavily 40+ years later.

[^5]: The Chauvet Cave paintings in France are dated to approximately 30,000-36,000 years ago, making them among the oldest known figurative art. See Clottes, J. (2003). "Chauvet Cave: The Art of Earliest Times." University of Utah Press.
